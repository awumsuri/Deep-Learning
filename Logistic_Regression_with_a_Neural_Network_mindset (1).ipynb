{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with a Neural Network mindset\n",
    "\n",
    "Welcome to your first (required) programming assignment! You will build a logistic regression classifier to recognize  cats. This assignment will step you through how to do this with a Neural Network mindset, and will also hone your intuitions about deep learning.\n",
    "\n",
    "**Instructions:**\n",
    "- Do not use loops (for/while) in your code, unless the instructions explicitly ask you to do so.\n",
    "- Use `np.dot(X,Y)` to calculate dot products.\n",
    "\n",
    "**You will learn to:**\n",
    "- Build the general architecture of a learning algorithm, including:\n",
    "    - Initializing parameters\n",
    "    - Calculating the cost function and its gradient\n",
    "    - Using an optimization algorithm (gradient descent) \n",
    "- Gather all three functions above into a main model function, in the right order.\n",
    "\n",
    "## Important Note on Submission to the AutoGrader\n",
    "\n",
    "Before submitting your assignment to the AutoGrader, please make sure you are not doing the following:\n",
    "\n",
    "1. You have not added any _extra_ `print` statement(s) in the assignment.\n",
    "2. You have not added any _extra_ code cell(s) in the assignment.\n",
    "3. You have not changed any of the function parameters.\n",
    "4. You are not using any global variables inside your graded exercises. Unless specifically instructed to do so, please refrain from it and use the local variables instead.\n",
    "5. You are not changing the assignment code where it is not required, like creating _extra_ variables.\n",
    "\n",
    "If you do any of the following, you will get something like, `Grader not found` (or similarly unexpected) error upon submitting your assignment. Before asking for help/debugging the errors in your assignment, check for these first. If this is the case, and you don't remember the changes you have made, you can get a fresh copy of the assignment by following these [instructions](https://www.coursera.org/learn/neural-networks-deep-learning/supplement/iLwon/h-ow-to-refresh-your-workspace)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [1 - Packages](#1)\n",
    "- [2 - Overview of the Problem set](#2)\n",
    "    - [Exercise 1](#ex-1)\n",
    "    - [Exercise 2](#ex-2)\n",
    "- [3 - General Architecture of the learning algorithm](#3)\n",
    "- [4 - Building the parts of our algorithm](#4)\n",
    "    - [4.1 - Helper functions](#4-1)\n",
    "        - [Exercise 3 - sigmoid](#ex-3)\n",
    "    - [4.2 - Initializing parameters](#4-2)\n",
    "        - [Exercise 4 - initialize_with_zeros](#ex-4)\n",
    "    - [4.3 - Forward and Backward propagation](#4-3)\n",
    "        - [Exercise 5 - propagate](#ex-5)\n",
    "    - [4.4 - Optimization](#4-4)\n",
    "        - [Exercise 6 - optimize](#ex-6)\n",
    "        - [Exercise 7 - predict](#ex-7)\n",
    "- [5 - Merge all functions into a model](#5)\n",
    "    - [Exercise 8 - model](#ex-8)\n",
    "- [6 - Further analysis (optional/ungraded exercise)](#6)\n",
    "- [7 - Test with your own image (optional/ungraded exercise)](#7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Packages ##\n",
    "\n",
    "First, let's run the cell below to import all the packages that you will need during this assignment. \n",
    "- [numpy](https://numpy.org/doc/1.20/) is the fundamental package for scientific computing with Python.\n",
    "- [h5py](http://www.h5py.org) is a common package to interact with a dataset that is stored on an H5 file.\n",
    "- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python.\n",
    "- [PIL](https://pillow.readthedocs.io/en/stable/) and [scipy](https://www.scipy.org/) are used here to test your model with your own picture at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from lr_utils import load_dataset\n",
    "from public_tests import *\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Overview of the Problem set ##\n",
    "\n",
    "**Problem Statement**: You are given a dataset (\"data.h5\") containing:\n",
    "    - a training set of m_train images labeled as cat (y=1) or non-cat (y=0)\n",
    "    - a test set of m_test images labeled as cat or non-cat\n",
    "    - each image is of shape (num_px, num_px, 3) where 3 is for the 3 channels (RGB). Thus, each image is square (height = num_px) and (width = num_px).\n",
    "\n",
    "You will build a simple image-recognition algorithm that can correctly classify pictures as cat or non-cat.\n",
    "\n",
    "Let's get more familiar with the dataset. Load the data by running the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data (cat/non-cat)\n",
    "train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We added \"_orig\" at the end of image datasets (train and test) because we are going to preprocess them. After preprocessing, we will end up with train_set_x and test_set_x (the labels train_set_y and test_set_y don't need any preprocessing).\n",
    "\n",
    "Each line of your train_set_x_orig and test_set_x_orig is an array representing an image. You can visualize an example by running the following code. Feel free also to change the `index` value and re-run to see other images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = [0], it's a 'non-cat' picture.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of a picture\n",
    "index = 1\n",
    "plt.imshow(train_set_x_orig[index])\n",
    "print (\"y = \" + str(train_set_y[:, index]) + \", it's a '\" + classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") +  \"' picture.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many software bugs in deep learning come from having matrix/vector dimensions that don't fit. If you can keep your matrix/vector dimensions straight you will go a long way toward eliminating many bugs. \n",
    "\n",
    "<a name='ex-1'></a>\n",
    "### Exercise 1\n",
    "Find the values for:\n",
    "    - m_train (number of training examples)\n",
    "    - m_test (number of test examples)\n",
    "    - num_px (= height = width of a training image)\n",
    "Remember that `train_set_x_orig` is a numpy-array of shape (m_train, num_px, num_px, 3). For instance, you can access `m_train` by writing `train_set_x_orig.shape[0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "921fe679a632ec7ec9963069fa405725",
     "grade": false,
     "grade_id": "cell-c4e7e9c1f174eb83",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape (50, 64, 64, 3)\n",
      "Number of training examples: m_train = 209\n",
      "Number of testing examples: m_test = 50\n",
      "Height/Width of each image: num_px = 64\n",
      "Each image is of size: (64, 64, 3)\n",
      "train_set_x shape: (209, 64, 64, 3)\n",
      "train_set_y shape: (1, 209)\n",
      "test_set_x shape: (50, 64, 64, 3)\n",
      "test_set_y shape: (1, 50)\n"
     ]
    }
   ],
   "source": [
    "#(≈ 3 lines of code)\n",
    "# m_train = \n",
    "# m_test = \n",
    "# num_px = \n",
    "# YOUR CODE STARTS HERE\n",
    "print(\"shape \" + str(test_set_x_orig.shape))\n",
    "m_test = test_set_x_orig.shape[0]\n",
    "m_train = train_set_x_orig.shape[0]\n",
    "num_px = train_set_x_orig.shape[2]\n",
    "\n",
    "# YOUR CODE ENDS HERE\n",
    "\n",
    "print (\"Number of training examples: m_train = \" + str(m_train))\n",
    "print (\"Number of testing examples: m_test = \" + str(m_test))\n",
    "print (\"Height/Width of each image: num_px = \" + str(num_px))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"train_set_x shape: \" + str(train_set_x_orig.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x shape: \" + str(test_set_x_orig.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output for m_train, m_test and num_px**: \n",
    "<table style=\"width:15%\">\n",
    "  <tr>\n",
    "    <td> m_train </td>\n",
    "    <td> 209 </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>m_test</td>\n",
    "    <td> 50 </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>num_px</td>\n",
    "    <td> 64 </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, you should now reshape images of shape (num_px, num_px, 3) in a numpy-array of shape (num_px $*$ num_px $*$ 3, 1). After this, our training (and test) dataset is a numpy-array where each column represents a flattened image. There should be m_train (respectively m_test) columns.\n",
    "\n",
    "<a name='ex-2'></a>\n",
    "### Exercise 2\n",
    "Reshape the training and test data sets so that images of size (num_px, num_px, 3) are flattened into single vectors of shape (num\\_px $*$ num\\_px $*$ 3, 1).\n",
    "\n",
    "A trick when you want to flatten a matrix X of shape (a,b,c,d) to a matrix X_flatten of shape (b$*$c$*$d, a) is to use: \n",
    "```python\n",
    "X_flatten = X.reshape(X.shape[0], -1).T      # X.T is the transpose of X\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5a2aa62bdd8c01450111b758ef159aec",
     "grade": false,
     "grade_id": "cell-0f43921062c34e50",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_x_flatten shape: (12288, 209)\n",
      "train_set_x shape: [196 192 190 193 186 182 188 179 174 213]\n",
      "train_set_x_flatten shape: (12288, 209)\n",
      "train_set_y shape: (1, 209)\n",
      "test_set_x_flatten shape: (12288, 50)\n",
      "test_set_y shape: (1, 50)\n"
     ]
    }
   ],
   "source": [
    "# Reshape the training and test examples\n",
    "#(≈ 2 lines of code)\n",
    "# train_set_x_flatten = ...\n",
    "# test_set_x_flatten = ...\n",
    "# YOUR CODE STARTS HERE\n",
    "train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T\n",
    "test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T\n",
    "print (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\n",
    "print (\"train_set_x shape: \" + str(train_set_x_flatten[0:10, 1]))\n",
    "\n",
    "# YOUR CODE ENDS HERE\n",
    "\n",
    "# Check that the first 10 pixels of the second image are in the correct place\n",
    "assert np.alltrue(train_set_x_flatten[0:10, 1] == [196, 192, 190, 193, 186, 182, 188, 179, 174, 213]), \"Wrong solution. Use (X.shape[0], -1).T.\"\n",
    "assert np.alltrue(test_set_x_flatten[0:10, 1] == [115, 110, 111, 137, 129, 129, 155, 146, 145, 159]), \"Wrong solution. Use (X.shape[0], -1).T.\"\n",
    "\n",
    "print (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table style=\"width:35%\">\n",
    "  <tr>\n",
    "    <td>train_set_x_flatten shape</td>\n",
    "    <td> (12288, 209)</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>train_set_y shape</td>\n",
    "    <td>(1, 209)</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>test_set_x_flatten shape</td>\n",
    "    <td>(12288, 50)</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>test_set_y shape</td>\n",
    "    <td>(1, 50)</td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To represent color images, the red, green and blue channels (RGB) must be specified for each pixel, and so the pixel value is actually a vector of three numbers ranging from 0 to 255.\n",
    "\n",
    "One common preprocessing step in machine learning is to center and standardize your dataset, meaning that you substract the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole numpy array. But for picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the dataset by 255 (the maximum value of a pixel channel).\n",
    "\n",
    "<!-- During the training of your model, you're going to multiply weights and add biases to some initial inputs in order to observe neuron activations. Then you backpropogate with the gradients to train the model. But, it is extremely important for each feature to have a similar range such that our gradients don't explode. You will see that more in detail later in the lectures. !--> \n",
    "\n",
    "Let's standardize our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x = train_set_x_flatten / 255.\n",
    "test_set_x = test_set_x_flatten / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "    \n",
    "**What you need to remember:**\n",
    "\n",
    "Common steps for pre-processing a new dataset are:\n",
    "- Figure out the dimensions and shapes of the problem (m_train, m_test, num_px, ...)\n",
    "- Reshape the datasets such that each example is now a vector of size (num_px \\* num_px \\* 3, 1)\n",
    "- \"Standardize\" the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - General Architecture of the learning algorithm ##\n",
    "\n",
    "It's time to design a simple algorithm to distinguish cat images from non-cat images.\n",
    "\n",
    "You will build a Logistic Regression, using a Neural Network mindset. The following Figure explains why **Logistic Regression is actually a very simple Neural Network!**\n",
    "\n",
    "<img src=\"images/LogReg_kiank.png\" style=\"width:650px;height:400px;\">\n",
    "\n",
    "**Mathematical expression of the algorithm**:\n",
    "\n",
    "For one example $x^{(i)}$:\n",
    "$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n",
    "$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}$$ \n",
    "$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}$$\n",
    "\n",
    "The cost is then computed by summing over all training examples:\n",
    "$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{6}$$\n",
    "\n",
    "**Key steps**:\n",
    "In this exercise, you will carry out the following steps: \n",
    "    - Initialize the parameters of the model\n",
    "    - Learn the parameters for the model by minimizing the cost  \n",
    "    - Use the learned parameters to make predictions (on the test set)\n",
    "    - Analyse the results and conclude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 - Building the parts of our algorithm ## \n",
    "\n",
    "The main steps for building a Neural Network are:\n",
    "1. Define the model structure (such as number of input features) \n",
    "2. Initialize the model's parameters\n",
    "3. Loop:\n",
    "    - Calculate current loss (forward propagation)\n",
    "    - Calculate current gradient (backward propagation)\n",
    "    - Update parameters (gradient descent)\n",
    "\n",
    "You often build 1-3 separately and integrate them into one function we call `model()`.\n",
    "\n",
    "<a name='4-1'></a>\n",
    "### 4.1 - Helper functions\n",
    "\n",
    "<a name='ex-3'></a>\n",
    "### Exercise 3 - sigmoid\n",
    "Using your code from \"Python Basics\", implement `sigmoid()`. As you've seen in the figure above, you need to compute $sigmoid(z) = \\frac{1}{1 + e^{-z}}$ for $z = w^T x + b$ to make predictions. Use np.exp()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "239ab1cf1028b721fd14f31b8103c40d",
     "grade": false,
     "grade_id": "cell-520521c430352f3b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sigmoid\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "\n",
    "    #(≈ 1 line of code)\n",
    "    # s = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    s = 1/(1 + np.exp(-z))\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0483e6820669111a9c5914d8b24bc315",
     "grade": true,
     "grade_id": "cell-30ea3151cab9c491",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid([0, 2]) = [0.5        0.88079708]\n",
      "\u001b[92mAll tests passed!\n"
     ]
    }
   ],
   "source": [
    "print (\"sigmoid([0, 2]) = \" + str(sigmoid(np.array([0,2]))))\n",
    "\n",
    "sigmoid_test(sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.62245933 0.5        0.88079708]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0.5, 0, 2.0])\n",
    "output = sigmoid(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4-2'></a>\n",
    "### 4.2 - Initializing parameters\n",
    "\n",
    "<a name='ex-4'></a>\n",
    "### Exercise 4 - initialize_with_zeros\n",
    "Implement parameter initialization in the cell below. You have to initialize w as a vector of zeros. If you don't know what numpy function to use, look up np.zeros() in the Numpy library's documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c4a37e375a85ddab7274a33abf46bb7c",
     "grade": false,
     "grade_id": "cell-befa9335e479864e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_with_zeros\n",
    "\n",
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"\n",
    "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
    "    \n",
    "    Argument:\n",
    "    dim -- size of the w vector we want (or number of parameters in this case)\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim, 1)\n",
    "    b -- initialized scalar (corresponds to the bias) of type float\n",
    "    \"\"\"\n",
    "    \n",
    "    # (≈ 2 lines of code)\n",
    "    # w = ...\n",
    "    # b = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    print(\"dim: \" + str(dim))\n",
    "    w = np.zeros((dim,1))\n",
    "    b = 0.0\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a4c13b0eafa46ca94de21b41faea8c58",
     "grade": true,
     "grade_id": "cell-a3b6699f145f3a3f",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim: 2\n",
      "w = [[0.]\n",
      " [0.]]\n",
      "b = 0.0\n",
      "dim: 3\n",
      "\u001b[92mFirst test passed!\n",
      "dim: 4\n",
      "\u001b[92mSecond test passed!\n"
     ]
    }
   ],
   "source": [
    "dim = 2\n",
    "w, b = initialize_with_zeros(dim)\n",
    "\n",
    "assert type(b) == float\n",
    "print (\"w = \" + str(w))\n",
    "print (\"b = \" + str(b))\n",
    "\n",
    "initialize_with_zeros_test_1(initialize_with_zeros)\n",
    "initialize_with_zeros_test_2(initialize_with_zeros)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ff9081e51809aef5e93bc1c21dc9b7b",
     "grade": false,
     "grade_id": "cell-11af17e28077b3d3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: propagate\n",
    "\n",
    "def propagate(w, b, X, Y):\n",
    "#     print(\"w: \" + str(w))\n",
    "#     print(\"b: \" + str(b))\n",
    "#     print(\"X: \" + str(X))\n",
    "#     print(\"w.TX\" + str(np.dot(w.T, X)))\n",
    "    \n",
    "    \"\"\"\n",
    "    Implement the cost function and its gradient for the propagation explained above\n",
    "\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "    \n",
    "    Tips:\n",
    "    - Write your code step by step for the propagation. np.log(), np.dot()\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # FORWARD PROPAGATION (FROM X TO COST)\n",
    "    #(≈ 2 lines of code)\n",
    "    # compute activation\n",
    "    # A = ...\n",
    "    # compute cost by using np.dot to perform multiplication. \n",
    "    # And don't use loops for the sum.\n",
    "    # cost = ...                                \n",
    "    # YOUR CODE STARTS HERE  \n",
    "    \n",
    "    A = sigmoid(np.dot(w.T,X) +  b )   \n",
    "    cost = -1/m * np.sum(Y * np.log(A) + (1-Y) * np.log(1 - A))   \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "\n",
    "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "    #(≈ 2 lines of code)\n",
    "    # dw = ...\n",
    "    # db = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    dw = 1/m * np.dot(X,(A - Y).T)\n",
    "    db = 1/m * np.sum(A - Y)\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    # cost = np.squeeze(np.array(cost))\n",
    "\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4-3'></a>\n",
    "### 4.3 - Forward and Backward propagation\n",
    "\n",
    "Now that your parameters are initialized, you can do the \"forward\" and \"backward\" propagation steps for learning the parameters.\n",
    "\n",
    "<a name='ex-5'></a>\n",
    "### Exercise 5 - propagate\n",
    "Implement a function `propagate()` that computes the cost function and its gradient.\n",
    "\n",
    "**Hints**:\n",
    "\n",
    "Forward Propagation:\n",
    "- You get X\n",
    "- You compute $A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$\n",
    "- You calculate the cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)}))$\n",
    "\n",
    "Here are the two formulas you will be using: \n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}$$\n",
    "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b8a1a4b1ff8d70ac609d721490b4d826",
     "grade": true,
     "grade_id": "cell-d1594d75b61dd554",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dw = [[ 0.25071532]\n",
      " [-0.06604096]]\n",
      "db = -0.1250040450043965\n",
      "cost = 0.15900537707692405\n",
      "\u001b[92mAll tests passed!\n"
     ]
    }
   ],
   "source": [
    "w =  np.array([[1.], [2]])\n",
    "b = 1.5\n",
    "X = np.array([[1., -2., -1.], [3., 0.5, -3.2]])\n",
    "Y = np.array([[1, 1, 0]])\n",
    "grads, cost = propagate(w, b, X, Y)\n",
    "\n",
    "assert type(grads[\"dw\"]) == np.ndarray\n",
    "assert grads[\"dw\"].shape == (2, 1)\n",
    "assert type(grads[\"db\"]) == np.float64\n",
    "\n",
    "\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "print (\"cost = \" + str(cost))\n",
    "\n",
    "propagate_test(propagate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "\n",
    "```\n",
    "dw = [[ 0.25071532]\n",
    " [-0.06604096]]\n",
    "db = -0.1250040450043965\n",
    "cost = 0.15900537707692405\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4-4'></a>\n",
    "### 4.4 - Optimization\n",
    "- You have initialized your parameters.\n",
    "- You are also able to compute a cost function and its gradient.\n",
    "- Now, you want to update the parameters using gradient descent.\n",
    "\n",
    "<a name='ex-6'></a>\n",
    "### Exercise 6 - optimize\n",
    "Write down the optimization function. The goal is to learn $w$ and $b$ by minimizing the cost function $J$. For a parameter $\\theta$, the update rule is $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "49d9b4c1a780bf141c8eb48e06cbb494",
     "grade": false,
     "grade_id": "cell-616d6883e807448d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: optimize\n",
    "\n",
    "def optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=True):\n",
    "    print_cost = True\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    Tips:\n",
    "    You basically need to write down two steps and iterate through them:\n",
    "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
    "        2) Update the parameters using gradient descent rule for w and b.\n",
    "    \"\"\"\n",
    "    \n",
    "    w = copy.deepcopy(w)\n",
    "    b = copy.deepcopy(b)\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # (≈ 1 lines of code)\n",
    "        # Cost and gradient calculation \n",
    "        # grads, cost = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "       \n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]        \n",
    "        \n",
    "        # update rule (≈ 2 lines of code)\n",
    "        # w = ...\n",
    "        # b = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "        # Record the costs\n",
    "        if i % 2 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "            # Print the cost every 100 training iterations\n",
    "            if print_cost:\n",
    "                print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b65a5c90f86a990614156e41f64b4678",
     "grade": true,
     "grade_id": "cell-8e3d43fbb82a8901",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.582672\n",
      "w = [[0.35627617]\n",
      " [0.60199214]]\n",
      "b = -0.14956979978997245\n",
      "dw = [[-0.21189539]\n",
      " [-0.33376766]]\n",
      "db = -0.13290329100668047\n",
      "Costs = [array(0.5826722)]\n",
      "Cost after iteration 0: 5.801545\n",
      "Cost after iteration 100: 0.310571\n",
      "\u001b[92mAll tests passed!\n"
     ]
    }
   ],
   "source": [
    "params, grads, costs = optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False)\n",
    "\n",
    "print (\"w = \" + str(params[\"w\"]))\n",
    "print (\"b = \" + str(params[\"b\"]))\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "print(\"Costs = \" + str(costs))\n",
    "\n",
    "optimize_test(optimize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-7'></a>\n",
    "### Exercise 7 - predict\n",
    "The previous function will output the learned w and b. We are able to use w and b to predict the labels for a dataset X. Implement the `predict()` function. There are two steps to computing predictions:\n",
    "\n",
    "1. Calculate $\\hat{Y} = A = \\sigma(w^T X + b)$\n",
    "\n",
    "2. Convert the entries of a into 0 (if activation <= 0.5) or 1 (if activation > 0.5), stores the predictions in a vector `Y_prediction`. If you wish, you can use an `if`/`else` statement in a `for` loop (though there is also a way to vectorize this). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e56419b97ebf382a8f93ac2873988887",
     "grade": false,
     "grade_id": "cell-d6f924f49c51dc2f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: predict\n",
    "\n",
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1, m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n",
    "    #(≈ 1 line of code)\n",
    "    # A = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        \n",
    "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
    "        #(≈ 4 lines of code)\n",
    "        # if A[0, i] > ____ :\n",
    "        #     Y_prediction[0,i] = \n",
    "        # else:\n",
    "        #     Y_prediction[0,i] = \n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        if A[0,i] > 0.5:\n",
    "            Y_prediction[0,i] = 1\n",
    "        else:\n",
    "            Y_prediction[0,i] = 0\n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e3ea12608f15798d542a07c1bc9f561b",
     "grade": true,
     "grade_id": "cell-90b1fb967269548c",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions = [[1. 1. 0.]]\n",
      "\u001b[92mAll tests passed!\n"
     ]
    }
   ],
   "source": [
    "w = np.array([[0.1124579], [0.23106775]])\n",
    "b = -0.3\n",
    "X = np.array([[1., -1.1, -3.2],[1.2, 2., 0.1]])\n",
    "print (\"predictions = \" + str(predict(w, b, X)))\n",
    "\n",
    "predict_test(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "**What to remember:**\n",
    "    \n",
    "You've implemented several functions that:\n",
    "- Initialize (w,b)\n",
    "- Optimize the loss iteratively to learn parameters (w,b):\n",
    "    - Computing the cost and its gradient \n",
    "    - Updating the parameters using gradient descent\n",
    "- Use the learned (w,b) to predict the labels for a given set of examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## 5 - Merge all functions into a model ##\n",
    "\n",
    "You will now see how the overall model is structured by putting together all the building blocks (functions implemented in the previous parts) together, in the right order.\n",
    "\n",
    "<a name='ex-8'></a>\n",
    "### Exercise 8 - model\n",
    "Implement the model function. Use the following notation:\n",
    "    - Y_prediction_test for your predictions on the test set\n",
    "    - Y_prediction_train for your predictions on the train set\n",
    "    - parameters, grads, costs for the outputs of optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f23cca6cfb750397e5d2ac44977e2c2a",
     "grade": false,
     "grade_id": "cell-6dcba5967c4cbf8c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model\n",
    "\n",
    "def model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function you've implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
    "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
    "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
    "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
    "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to True to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    # (≈ 1 line of code)   \n",
    "    # initialize parameters with zeros \n",
    "    # w, b = ...\n",
    "    \n",
    "    #(≈ 1 line of code)\n",
    "    # Gradient descent \n",
    "    # params, grads, costs = ...\n",
    "    \n",
    "    # Retrieve parameters w and b from dictionary \"params\"\n",
    "    # w = ...\n",
    "    # b = ...\n",
    "    \n",
    "    # Predict test/train set examples (≈ 2 lines of code)\n",
    "    # Y_prediction_test = ...\n",
    "    # Y_prediction_train = ...\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    m = X_train.shape[0]\n",
    "    w, b = initialize_with_zeros(m)\n",
    "    \n",
    "    print(\"X_train shape:\", X_train.shape)\n",
    "    print(\"Y_train shape:\", Y_train.shape)\n",
    "    print(\"X_test shape:\", X_test.shape)\n",
    "    print(\"Y_test shape:\", Y_test.shape)\n",
    "        \n",
    "    params, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "    \n",
    "    w = params[\"w\"]\n",
    "    b = params[\"b\"]\n",
    "    \n",
    "    Y_prediction_test = predict(w, b, X_test)\n",
    "    Y_prediction_train = predict(w, b, X_train)\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "\n",
    "    # Print train/test Errors\n",
    "    if print_cost:\n",
    "        print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "        print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    \n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b468bc5ddf6ecc5c7dbcb9a02cfe0216",
     "grade": true,
     "grade_id": "cell-4170e070f3cde17e",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim: 4\n",
      "X_train shape: (4, 7)\n",
      "Y_train shape: (1, 7)\n",
      "X_test shape: (4, 3)\n",
      "Y_test shape: (3,)\n",
      "Cost after iteration 0: 0.693147\n",
      "\u001b[92mAll tests passed!\n"
     ]
    }
   ],
   "source": [
    "from public_tests import *\n",
    "\n",
    "model_test(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you pass all the tests, run the following cell to train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim: 12288\n",
      "X_train shape: (12288, 209)\n",
      "Y_train shape: (1, 209)\n",
      "X_test shape: (12288, 50)\n",
      "Y_test shape: (1, 50)\n",
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 2: 0.753154\n",
      "Cost after iteration 4: 0.768564\n",
      "Cost after iteration 6: 0.755613\n",
      "Cost after iteration 8: 0.751485\n",
      "Cost after iteration 10: 0.744940\n",
      "Cost after iteration 12: 0.739538\n",
      "Cost after iteration 14: 0.734114\n",
      "Cost after iteration 16: 0.728988\n",
      "Cost after iteration 18: 0.724030\n",
      "Cost after iteration 20: 0.719249\n",
      "Cost after iteration 22: 0.714618\n",
      "Cost after iteration 24: 0.710124\n",
      "Cost after iteration 26: 0.705753\n",
      "Cost after iteration 28: 0.701493\n",
      "Cost after iteration 30: 0.697336\n",
      "Cost after iteration 32: 0.693272\n",
      "Cost after iteration 34: 0.689294\n",
      "Cost after iteration 36: 0.685395\n",
      "Cost after iteration 38: 0.681570\n",
      "Cost after iteration 40: 0.677814\n",
      "Cost after iteration 42: 0.674123\n",
      "Cost after iteration 44: 0.670491\n",
      "Cost after iteration 46: 0.666917\n",
      "Cost after iteration 48: 0.663395\n",
      "Cost after iteration 50: 0.659925\n",
      "Cost after iteration 52: 0.656503\n",
      "Cost after iteration 54: 0.653126\n",
      "Cost after iteration 56: 0.649792\n",
      "Cost after iteration 58: 0.646500\n",
      "Cost after iteration 60: 0.643248\n",
      "Cost after iteration 62: 0.640033\n",
      "Cost after iteration 64: 0.636854\n",
      "Cost after iteration 66: 0.633710\n",
      "Cost after iteration 68: 0.630599\n",
      "Cost after iteration 70: 0.627521\n",
      "Cost after iteration 72: 0.624473\n",
      "Cost after iteration 74: 0.621455\n",
      "Cost after iteration 76: 0.618465\n",
      "Cost after iteration 78: 0.615503\n",
      "Cost after iteration 80: 0.612568\n",
      "Cost after iteration 82: 0.609658\n",
      "Cost after iteration 84: 0.606773\n",
      "Cost after iteration 86: 0.603913\n",
      "Cost after iteration 88: 0.601076\n",
      "Cost after iteration 90: 0.598262\n",
      "Cost after iteration 92: 0.595469\n",
      "Cost after iteration 94: 0.592699\n",
      "Cost after iteration 96: 0.589948\n",
      "Cost after iteration 98: 0.587219\n",
      "Cost after iteration 100: 0.584508\n",
      "Cost after iteration 102: 0.581817\n",
      "Cost after iteration 104: 0.579145\n",
      "Cost after iteration 106: 0.576491\n",
      "Cost after iteration 108: 0.573854\n",
      "Cost after iteration 110: 0.571235\n",
      "Cost after iteration 112: 0.568633\n",
      "Cost after iteration 114: 0.566047\n",
      "Cost after iteration 116: 0.563478\n",
      "Cost after iteration 118: 0.560924\n",
      "Cost after iteration 120: 0.558386\n",
      "Cost after iteration 122: 0.555863\n",
      "Cost after iteration 124: 0.553354\n",
      "Cost after iteration 126: 0.550860\n",
      "Cost after iteration 128: 0.548381\n",
      "Cost after iteration 130: 0.545915\n",
      "Cost after iteration 132: 0.543463\n",
      "Cost after iteration 134: 0.541025\n",
      "Cost after iteration 136: 0.538600\n",
      "Cost after iteration 138: 0.536188\n",
      "Cost after iteration 140: 0.533788\n",
      "Cost after iteration 142: 0.531401\n",
      "Cost after iteration 144: 0.529027\n",
      "Cost after iteration 146: 0.526665\n",
      "Cost after iteration 148: 0.524314\n",
      "Cost after iteration 150: 0.521976\n",
      "Cost after iteration 152: 0.519649\n",
      "Cost after iteration 154: 0.517333\n",
      "Cost after iteration 156: 0.515029\n",
      "Cost after iteration 158: 0.512736\n",
      "Cost after iteration 160: 0.510454\n",
      "Cost after iteration 162: 0.508183\n",
      "Cost after iteration 164: 0.505922\n",
      "Cost after iteration 166: 0.503673\n",
      "Cost after iteration 168: 0.501433\n",
      "Cost after iteration 170: 0.499204\n",
      "Cost after iteration 172: 0.496986\n",
      "Cost after iteration 174: 0.494777\n",
      "Cost after iteration 176: 0.492578\n",
      "Cost after iteration 178: 0.490390\n",
      "Cost after iteration 180: 0.488211\n",
      "Cost after iteration 182: 0.486042\n",
      "Cost after iteration 184: 0.483883\n",
      "Cost after iteration 186: 0.481733\n",
      "Cost after iteration 188: 0.479593\n",
      "Cost after iteration 190: 0.477462\n",
      "Cost after iteration 192: 0.475341\n",
      "Cost after iteration 194: 0.473229\n",
      "Cost after iteration 196: 0.471127\n",
      "Cost after iteration 198: 0.469033\n",
      "Cost after iteration 200: 0.466949\n",
      "Cost after iteration 202: 0.464874\n",
      "Cost after iteration 204: 0.462808\n",
      "Cost after iteration 206: 0.460751\n",
      "Cost after iteration 208: 0.458704\n",
      "Cost after iteration 210: 0.456665\n",
      "Cost after iteration 212: 0.454635\n",
      "Cost after iteration 214: 0.452615\n",
      "Cost after iteration 216: 0.450603\n",
      "Cost after iteration 218: 0.448601\n",
      "Cost after iteration 220: 0.446607\n",
      "Cost after iteration 222: 0.444623\n",
      "Cost after iteration 224: 0.442648\n",
      "Cost after iteration 226: 0.440682\n",
      "Cost after iteration 228: 0.438725\n",
      "Cost after iteration 230: 0.436777\n",
      "Cost after iteration 232: 0.434839\n",
      "Cost after iteration 234: 0.432910\n",
      "Cost after iteration 236: 0.430991\n",
      "Cost after iteration 238: 0.429081\n",
      "Cost after iteration 240: 0.427181\n",
      "Cost after iteration 242: 0.425291\n",
      "Cost after iteration 244: 0.423411\n",
      "Cost after iteration 246: 0.421541\n",
      "Cost after iteration 248: 0.419681\n",
      "Cost after iteration 250: 0.417832\n",
      "Cost after iteration 252: 0.415993\n",
      "Cost after iteration 254: 0.414165\n",
      "Cost after iteration 256: 0.412349\n",
      "Cost after iteration 258: 0.410544\n",
      "Cost after iteration 260: 0.408750\n",
      "Cost after iteration 262: 0.406969\n",
      "Cost after iteration 264: 0.405199\n",
      "Cost after iteration 266: 0.403443\n",
      "Cost after iteration 268: 0.401699\n",
      "Cost after iteration 270: 0.399969\n",
      "Cost after iteration 272: 0.398253\n",
      "Cost after iteration 274: 0.396552\n",
      "Cost after iteration 276: 0.394865\n",
      "Cost after iteration 278: 0.393193\n",
      "Cost after iteration 280: 0.391538\n",
      "Cost after iteration 282: 0.389899\n",
      "Cost after iteration 284: 0.388277\n",
      "Cost after iteration 286: 0.386673\n",
      "Cost after iteration 288: 0.385088\n",
      "Cost after iteration 290: 0.383522\n",
      "Cost after iteration 292: 0.381976\n",
      "Cost after iteration 294: 0.380450\n",
      "Cost after iteration 296: 0.378946\n",
      "Cost after iteration 298: 0.377465\n",
      "Cost after iteration 300: 0.376007\n",
      "Cost after iteration 302: 0.374573\n",
      "Cost after iteration 304: 0.373163\n",
      "Cost after iteration 306: 0.371779\n",
      "Cost after iteration 308: 0.370422\n",
      "Cost after iteration 310: 0.369092\n",
      "Cost after iteration 312: 0.367789\n",
      "Cost after iteration 314: 0.366515\n",
      "Cost after iteration 316: 0.365270\n",
      "Cost after iteration 318: 0.364054\n",
      "Cost after iteration 320: 0.362867\n",
      "Cost after iteration 322: 0.361711\n",
      "Cost after iteration 324: 0.360584\n",
      "Cost after iteration 326: 0.359487\n",
      "Cost after iteration 328: 0.358420\n",
      "Cost after iteration 330: 0.357382\n",
      "Cost after iteration 332: 0.356373\n",
      "Cost after iteration 334: 0.355392\n",
      "Cost after iteration 336: 0.354438\n",
      "Cost after iteration 338: 0.353511\n",
      "Cost after iteration 340: 0.352608\n",
      "Cost after iteration 342: 0.351730\n",
      "Cost after iteration 344: 0.350874\n",
      "Cost after iteration 346: 0.350039\n",
      "Cost after iteration 348: 0.349224\n",
      "Cost after iteration 350: 0.348427\n",
      "Cost after iteration 352: 0.347647\n",
      "Cost after iteration 354: 0.346883\n",
      "Cost after iteration 356: 0.346133\n",
      "Cost after iteration 358: 0.345396\n",
      "Cost after iteration 360: 0.344671\n",
      "Cost after iteration 362: 0.343956\n",
      "Cost after iteration 364: 0.343250\n",
      "Cost after iteration 366: 0.342553\n",
      "Cost after iteration 368: 0.341863\n",
      "Cost after iteration 370: 0.341180\n",
      "Cost after iteration 372: 0.340503\n",
      "Cost after iteration 374: 0.339832\n",
      "Cost after iteration 376: 0.339166\n",
      "Cost after iteration 378: 0.338505\n",
      "Cost after iteration 380: 0.337847\n",
      "Cost after iteration 382: 0.337194\n",
      "Cost after iteration 384: 0.336545\n",
      "Cost after iteration 386: 0.335899\n",
      "Cost after iteration 388: 0.335256\n",
      "Cost after iteration 390: 0.334616\n",
      "Cost after iteration 392: 0.333980\n",
      "Cost after iteration 394: 0.333346\n",
      "Cost after iteration 396: 0.332716\n",
      "Cost after iteration 398: 0.332088\n",
      "Cost after iteration 400: 0.331463\n",
      "Cost after iteration 402: 0.330841\n",
      "Cost after iteration 404: 0.330222\n",
      "Cost after iteration 406: 0.329605\n",
      "Cost after iteration 408: 0.328991\n",
      "Cost after iteration 410: 0.328379\n",
      "Cost after iteration 412: 0.327770\n",
      "Cost after iteration 414: 0.327164\n",
      "Cost after iteration 416: 0.326560\n",
      "Cost after iteration 418: 0.325959\n",
      "Cost after iteration 420: 0.325360\n",
      "Cost after iteration 422: 0.324764\n",
      "Cost after iteration 424: 0.324170\n",
      "Cost after iteration 426: 0.323578\n",
      "Cost after iteration 428: 0.322989\n",
      "Cost after iteration 430: 0.322403\n",
      "Cost after iteration 432: 0.321819\n",
      "Cost after iteration 434: 0.321237\n",
      "Cost after iteration 436: 0.320658\n",
      "Cost after iteration 438: 0.320081\n",
      "Cost after iteration 440: 0.319506\n",
      "Cost after iteration 442: 0.318933\n",
      "Cost after iteration 444: 0.318363\n",
      "Cost after iteration 446: 0.317796\n",
      "Cost after iteration 448: 0.317230\n",
      "Cost after iteration 450: 0.316667\n",
      "Cost after iteration 452: 0.316106\n",
      "Cost after iteration 454: 0.315547\n",
      "Cost after iteration 456: 0.314990\n",
      "Cost after iteration 458: 0.314436\n",
      "Cost after iteration 460: 0.313883\n",
      "Cost after iteration 462: 0.313333\n",
      "Cost after iteration 464: 0.312785\n",
      "Cost after iteration 466: 0.312240\n",
      "Cost after iteration 468: 0.311696\n",
      "Cost after iteration 470: 0.311154\n",
      "Cost after iteration 472: 0.310615\n",
      "Cost after iteration 474: 0.310077\n",
      "Cost after iteration 476: 0.309542\n",
      "Cost after iteration 478: 0.309008\n",
      "Cost after iteration 480: 0.308477\n",
      "Cost after iteration 482: 0.307948\n",
      "Cost after iteration 484: 0.307421\n",
      "Cost after iteration 486: 0.306895\n",
      "Cost after iteration 488: 0.306372\n",
      "Cost after iteration 490: 0.305851\n",
      "Cost after iteration 492: 0.305331\n",
      "Cost after iteration 494: 0.304814\n",
      "Cost after iteration 496: 0.304298\n",
      "Cost after iteration 498: 0.303785\n",
      "Cost after iteration 500: 0.303273\n",
      "Cost after iteration 502: 0.302763\n",
      "Cost after iteration 504: 0.302255\n",
      "Cost after iteration 506: 0.301749\n",
      "Cost after iteration 508: 0.301245\n",
      "Cost after iteration 510: 0.300743\n",
      "Cost after iteration 512: 0.300242\n",
      "Cost after iteration 514: 0.299744\n",
      "Cost after iteration 516: 0.299247\n",
      "Cost after iteration 518: 0.298752\n",
      "Cost after iteration 520: 0.298258\n",
      "Cost after iteration 522: 0.297767\n",
      "Cost after iteration 524: 0.297277\n",
      "Cost after iteration 526: 0.296789\n",
      "Cost after iteration 528: 0.296303\n",
      "Cost after iteration 530: 0.295819\n",
      "Cost after iteration 532: 0.295336\n",
      "Cost after iteration 534: 0.294855\n",
      "Cost after iteration 536: 0.294375\n",
      "Cost after iteration 538: 0.293898\n",
      "Cost after iteration 540: 0.293422\n",
      "Cost after iteration 542: 0.292948\n",
      "Cost after iteration 544: 0.292475\n",
      "Cost after iteration 546: 0.292004\n",
      "Cost after iteration 548: 0.291535\n",
      "Cost after iteration 550: 0.291067\n",
      "Cost after iteration 552: 0.290601\n",
      "Cost after iteration 554: 0.290137\n",
      "Cost after iteration 556: 0.289674\n",
      "Cost after iteration 558: 0.289212\n",
      "Cost after iteration 560: 0.288753\n",
      "Cost after iteration 562: 0.288295\n",
      "Cost after iteration 564: 0.287838\n",
      "Cost after iteration 566: 0.287383\n",
      "Cost after iteration 568: 0.286930\n",
      "Cost after iteration 570: 0.286478\n",
      "Cost after iteration 572: 0.286028\n",
      "Cost after iteration 574: 0.285579\n",
      "Cost after iteration 576: 0.285132\n",
      "Cost after iteration 578: 0.284686\n",
      "Cost after iteration 580: 0.284242\n",
      "Cost after iteration 582: 0.283799\n",
      "Cost after iteration 584: 0.283358\n",
      "Cost after iteration 586: 0.282918\n",
      "Cost after iteration 588: 0.282479\n",
      "Cost after iteration 590: 0.282043\n",
      "Cost after iteration 592: 0.281607\n",
      "Cost after iteration 594: 0.281173\n",
      "Cost after iteration 596: 0.280740\n",
      "Cost after iteration 598: 0.280309\n",
      "Cost after iteration 600: 0.279880\n",
      "Cost after iteration 602: 0.279451\n",
      "Cost after iteration 604: 0.279024\n",
      "Cost after iteration 606: 0.278599\n",
      "Cost after iteration 608: 0.278175\n",
      "Cost after iteration 610: 0.277752\n",
      "Cost after iteration 612: 0.277330\n",
      "Cost after iteration 614: 0.276910\n",
      "Cost after iteration 616: 0.276492\n",
      "Cost after iteration 618: 0.276074\n",
      "Cost after iteration 620: 0.275658\n",
      "Cost after iteration 622: 0.275244\n",
      "Cost after iteration 624: 0.274831\n",
      "Cost after iteration 626: 0.274418\n",
      "Cost after iteration 628: 0.274008\n",
      "Cost after iteration 630: 0.273598\n",
      "Cost after iteration 632: 0.273190\n",
      "Cost after iteration 634: 0.272784\n",
      "Cost after iteration 636: 0.272378\n",
      "Cost after iteration 638: 0.271974\n",
      "Cost after iteration 640: 0.271571\n",
      "Cost after iteration 642: 0.271169\n",
      "Cost after iteration 644: 0.270769\n",
      "Cost after iteration 646: 0.270370\n",
      "Cost after iteration 648: 0.269972\n",
      "Cost after iteration 650: 0.269575\n",
      "Cost after iteration 652: 0.269179\n",
      "Cost after iteration 654: 0.268785\n",
      "Cost after iteration 656: 0.268392\n",
      "Cost after iteration 658: 0.268000\n",
      "Cost after iteration 660: 0.267610\n",
      "Cost after iteration 662: 0.267220\n",
      "Cost after iteration 664: 0.266832\n",
      "Cost after iteration 666: 0.266445\n",
      "Cost after iteration 668: 0.266059\n",
      "Cost after iteration 670: 0.265675\n",
      "Cost after iteration 672: 0.265291\n",
      "Cost after iteration 674: 0.264909\n",
      "Cost after iteration 676: 0.264528\n",
      "Cost after iteration 678: 0.264148\n",
      "Cost after iteration 680: 0.263769\n",
      "Cost after iteration 682: 0.263391\n",
      "Cost after iteration 684: 0.263015\n",
      "Cost after iteration 686: 0.262639\n",
      "Cost after iteration 688: 0.262265\n",
      "Cost after iteration 690: 0.261892\n",
      "Cost after iteration 692: 0.261519\n",
      "Cost after iteration 694: 0.261148\n",
      "Cost after iteration 696: 0.260779\n",
      "Cost after iteration 698: 0.260410\n",
      "Cost after iteration 700: 0.260042\n",
      "Cost after iteration 702: 0.259676\n",
      "Cost after iteration 704: 0.259310\n",
      "Cost after iteration 706: 0.258946\n",
      "Cost after iteration 708: 0.258582\n",
      "Cost after iteration 710: 0.258220\n",
      "Cost after iteration 712: 0.257859\n",
      "Cost after iteration 714: 0.257498\n",
      "Cost after iteration 716: 0.257139\n",
      "Cost after iteration 718: 0.256781\n",
      "Cost after iteration 720: 0.256424\n",
      "Cost after iteration 722: 0.256068\n",
      "Cost after iteration 724: 0.255713\n",
      "Cost after iteration 726: 0.255359\n",
      "Cost after iteration 728: 0.255006\n",
      "Cost after iteration 730: 0.254654\n",
      "Cost after iteration 732: 0.254303\n",
      "Cost after iteration 734: 0.253953\n",
      "Cost after iteration 736: 0.253604\n",
      "Cost after iteration 738: 0.253257\n",
      "Cost after iteration 740: 0.252910\n",
      "Cost after iteration 742: 0.252564\n",
      "Cost after iteration 744: 0.252219\n",
      "Cost after iteration 746: 0.251875\n",
      "Cost after iteration 748: 0.251532\n",
      "Cost after iteration 750: 0.251190\n",
      "Cost after iteration 752: 0.250849\n",
      "Cost after iteration 754: 0.250508\n",
      "Cost after iteration 756: 0.250169\n",
      "Cost after iteration 758: 0.249831\n",
      "Cost after iteration 760: 0.249494\n",
      "Cost after iteration 762: 0.249157\n",
      "Cost after iteration 764: 0.248822\n",
      "Cost after iteration 766: 0.248488\n",
      "Cost after iteration 768: 0.248154\n",
      "Cost after iteration 770: 0.247821\n",
      "Cost after iteration 772: 0.247490\n",
      "Cost after iteration 774: 0.247159\n",
      "Cost after iteration 776: 0.246829\n",
      "Cost after iteration 778: 0.246500\n",
      "Cost after iteration 780: 0.246172\n",
      "Cost after iteration 782: 0.245845\n",
      "Cost after iteration 784: 0.245519\n",
      "Cost after iteration 786: 0.245193\n",
      "Cost after iteration 788: 0.244869\n",
      "Cost after iteration 790: 0.244545\n",
      "Cost after iteration 792: 0.244223\n",
      "Cost after iteration 794: 0.243901\n",
      "Cost after iteration 796: 0.243580\n",
      "Cost after iteration 798: 0.243260\n",
      "Cost after iteration 800: 0.242941\n",
      "Cost after iteration 802: 0.242622\n",
      "Cost after iteration 804: 0.242305\n",
      "Cost after iteration 806: 0.241988\n",
      "Cost after iteration 808: 0.241672\n",
      "Cost after iteration 810: 0.241357\n",
      "Cost after iteration 812: 0.241043\n",
      "Cost after iteration 814: 0.240730\n",
      "Cost after iteration 816: 0.240418\n",
      "Cost after iteration 818: 0.240106\n",
      "Cost after iteration 820: 0.239795\n",
      "Cost after iteration 822: 0.239485\n",
      "Cost after iteration 824: 0.239176\n",
      "Cost after iteration 826: 0.238868\n",
      "Cost after iteration 828: 0.238560\n",
      "Cost after iteration 830: 0.238254\n",
      "Cost after iteration 832: 0.237948\n",
      "Cost after iteration 834: 0.237643\n",
      "Cost after iteration 836: 0.237338\n",
      "Cost after iteration 838: 0.237035\n",
      "Cost after iteration 840: 0.236732\n",
      "Cost after iteration 842: 0.236430\n",
      "Cost after iteration 844: 0.236129\n",
      "Cost after iteration 846: 0.235829\n",
      "Cost after iteration 848: 0.235529\n",
      "Cost after iteration 850: 0.235231\n",
      "Cost after iteration 852: 0.234933\n",
      "Cost after iteration 854: 0.234635\n",
      "Cost after iteration 856: 0.234339\n",
      "Cost after iteration 858: 0.234043\n",
      "Cost after iteration 860: 0.233748\n",
      "Cost after iteration 862: 0.233454\n",
      "Cost after iteration 864: 0.233161\n",
      "Cost after iteration 866: 0.232868\n",
      "Cost after iteration 868: 0.232576\n",
      "Cost after iteration 870: 0.232285\n",
      "Cost after iteration 872: 0.231994\n",
      "Cost after iteration 874: 0.231705\n",
      "Cost after iteration 876: 0.231416\n",
      "Cost after iteration 878: 0.231127\n",
      "Cost after iteration 880: 0.230840\n",
      "Cost after iteration 882: 0.230553\n",
      "Cost after iteration 884: 0.230267\n",
      "Cost after iteration 886: 0.229982\n",
      "Cost after iteration 888: 0.229697\n",
      "Cost after iteration 890: 0.229413\n",
      "Cost after iteration 892: 0.229130\n",
      "Cost after iteration 894: 0.228847\n",
      "Cost after iteration 896: 0.228566\n",
      "Cost after iteration 898: 0.228285\n",
      "Cost after iteration 900: 0.228004\n",
      "Cost after iteration 902: 0.227725\n",
      "Cost after iteration 904: 0.227446\n",
      "Cost after iteration 906: 0.227167\n",
      "Cost after iteration 908: 0.226890\n",
      "Cost after iteration 910: 0.226613\n",
      "Cost after iteration 912: 0.226336\n",
      "Cost after iteration 914: 0.226061\n",
      "Cost after iteration 916: 0.225786\n",
      "Cost after iteration 918: 0.225512\n",
      "Cost after iteration 920: 0.225238\n",
      "Cost after iteration 922: 0.224965\n",
      "Cost after iteration 924: 0.224693\n",
      "Cost after iteration 926: 0.224422\n",
      "Cost after iteration 928: 0.224151\n",
      "Cost after iteration 930: 0.223881\n",
      "Cost after iteration 932: 0.223611\n",
      "Cost after iteration 934: 0.223342\n",
      "Cost after iteration 936: 0.223074\n",
      "Cost after iteration 938: 0.222806\n",
      "Cost after iteration 940: 0.222539\n",
      "Cost after iteration 942: 0.222273\n",
      "Cost after iteration 944: 0.222007\n",
      "Cost after iteration 946: 0.221742\n",
      "Cost after iteration 948: 0.221478\n",
      "Cost after iteration 950: 0.221214\n",
      "Cost after iteration 952: 0.220951\n",
      "Cost after iteration 954: 0.220689\n",
      "Cost after iteration 956: 0.220427\n",
      "Cost after iteration 958: 0.220165\n",
      "Cost after iteration 960: 0.219905\n",
      "Cost after iteration 962: 0.219645\n",
      "Cost after iteration 964: 0.219385\n",
      "Cost after iteration 966: 0.219127\n",
      "Cost after iteration 968: 0.218869\n",
      "Cost after iteration 970: 0.218611\n",
      "Cost after iteration 972: 0.218354\n",
      "Cost after iteration 974: 0.218098\n",
      "Cost after iteration 976: 0.217842\n",
      "Cost after iteration 978: 0.217587\n",
      "Cost after iteration 980: 0.217332\n",
      "Cost after iteration 982: 0.217078\n",
      "Cost after iteration 984: 0.216825\n",
      "Cost after iteration 986: 0.216572\n",
      "Cost after iteration 988: 0.216320\n",
      "Cost after iteration 990: 0.216069\n",
      "Cost after iteration 992: 0.215818\n",
      "Cost after iteration 994: 0.215567\n",
      "Cost after iteration 996: 0.215317\n",
      "Cost after iteration 998: 0.215068\n",
      "Cost after iteration 1000: 0.214820\n",
      "Cost after iteration 1002: 0.214571\n",
      "Cost after iteration 1004: 0.214324\n",
      "Cost after iteration 1006: 0.214077\n",
      "Cost after iteration 1008: 0.213831\n",
      "Cost after iteration 1010: 0.213585\n",
      "Cost after iteration 1012: 0.213340\n",
      "Cost after iteration 1014: 0.213095\n",
      "Cost after iteration 1016: 0.212851\n",
      "Cost after iteration 1018: 0.212607\n",
      "Cost after iteration 1020: 0.212364\n",
      "Cost after iteration 1022: 0.212122\n",
      "Cost after iteration 1024: 0.211880\n",
      "Cost after iteration 1026: 0.211639\n",
      "Cost after iteration 1028: 0.211398\n",
      "Cost after iteration 1030: 0.211157\n",
      "Cost after iteration 1032: 0.210918\n",
      "Cost after iteration 1034: 0.210679\n",
      "Cost after iteration 1036: 0.210440\n",
      "Cost after iteration 1038: 0.210202\n",
      "Cost after iteration 1040: 0.209964\n",
      "Cost after iteration 1042: 0.209727\n",
      "Cost after iteration 1044: 0.209491\n",
      "Cost after iteration 1046: 0.209255\n",
      "Cost after iteration 1048: 0.209019\n",
      "Cost after iteration 1050: 0.208785\n",
      "Cost after iteration 1052: 0.208550\n",
      "Cost after iteration 1054: 0.208316\n",
      "Cost after iteration 1056: 0.208083\n",
      "Cost after iteration 1058: 0.207850\n",
      "Cost after iteration 1060: 0.207618\n",
      "Cost after iteration 1062: 0.207386\n",
      "Cost after iteration 1064: 0.207155\n",
      "Cost after iteration 1066: 0.206924\n",
      "Cost after iteration 1068: 0.206694\n",
      "Cost after iteration 1070: 0.206464\n",
      "Cost after iteration 1072: 0.206235\n",
      "Cost after iteration 1074: 0.206006\n",
      "Cost after iteration 1076: 0.205778\n",
      "Cost after iteration 1078: 0.205550\n",
      "Cost after iteration 1080: 0.205323\n",
      "Cost after iteration 1082: 0.205096\n",
      "Cost after iteration 1084: 0.204870\n",
      "Cost after iteration 1086: 0.204645\n",
      "Cost after iteration 1088: 0.204419\n",
      "Cost after iteration 1090: 0.204195\n",
      "Cost after iteration 1092: 0.203970\n",
      "Cost after iteration 1094: 0.203747\n",
      "Cost after iteration 1096: 0.203523\n",
      "Cost after iteration 1098: 0.203300\n",
      "Cost after iteration 1100: 0.203078\n",
      "Cost after iteration 1102: 0.202856\n",
      "Cost after iteration 1104: 0.202635\n",
      "Cost after iteration 1106: 0.202414\n",
      "Cost after iteration 1108: 0.202194\n",
      "Cost after iteration 1110: 0.201974\n",
      "Cost after iteration 1112: 0.201754\n",
      "Cost after iteration 1114: 0.201535\n",
      "Cost after iteration 1116: 0.201317\n",
      "Cost after iteration 1118: 0.201099\n",
      "Cost after iteration 1120: 0.200881\n",
      "Cost after iteration 1122: 0.200664\n",
      "Cost after iteration 1124: 0.200448\n",
      "Cost after iteration 1126: 0.200231\n",
      "Cost after iteration 1128: 0.200016\n",
      "Cost after iteration 1130: 0.199800\n",
      "Cost after iteration 1132: 0.199586\n",
      "Cost after iteration 1134: 0.199371\n",
      "Cost after iteration 1136: 0.199157\n",
      "Cost after iteration 1138: 0.198944\n",
      "Cost after iteration 1140: 0.198731\n",
      "Cost after iteration 1142: 0.198519\n",
      "Cost after iteration 1144: 0.198306\n",
      "Cost after iteration 1146: 0.198095\n",
      "Cost after iteration 1148: 0.197884\n",
      "Cost after iteration 1150: 0.197673\n",
      "Cost after iteration 1152: 0.197463\n",
      "Cost after iteration 1154: 0.197253\n",
      "Cost after iteration 1156: 0.197043\n",
      "Cost after iteration 1158: 0.196834\n",
      "Cost after iteration 1160: 0.196626\n",
      "Cost after iteration 1162: 0.196418\n",
      "Cost after iteration 1164: 0.196210\n",
      "Cost after iteration 1166: 0.196003\n",
      "Cost after iteration 1168: 0.195796\n",
      "Cost after iteration 1170: 0.195589\n",
      "Cost after iteration 1172: 0.195384\n",
      "Cost after iteration 1174: 0.195178\n",
      "Cost after iteration 1176: 0.194973\n",
      "Cost after iteration 1178: 0.194768\n",
      "Cost after iteration 1180: 0.194564\n",
      "Cost after iteration 1182: 0.194360\n",
      "Cost after iteration 1184: 0.194157\n",
      "Cost after iteration 1186: 0.193954\n",
      "Cost after iteration 1188: 0.193751\n",
      "Cost after iteration 1190: 0.193549\n",
      "Cost after iteration 1192: 0.193347\n",
      "Cost after iteration 1194: 0.193146\n",
      "Cost after iteration 1196: 0.192945\n",
      "Cost after iteration 1198: 0.192744\n",
      "Cost after iteration 1200: 0.192544\n",
      "Cost after iteration 1202: 0.192345\n",
      "Cost after iteration 1204: 0.192145\n",
      "Cost after iteration 1206: 0.191946\n",
      "Cost after iteration 1208: 0.191748\n",
      "Cost after iteration 1210: 0.191550\n",
      "Cost after iteration 1212: 0.191352\n",
      "Cost after iteration 1214: 0.191155\n",
      "Cost after iteration 1216: 0.190958\n",
      "Cost after iteration 1218: 0.190762\n",
      "Cost after iteration 1220: 0.190565\n",
      "Cost after iteration 1222: 0.190370\n",
      "Cost after iteration 1224: 0.190174\n",
      "Cost after iteration 1226: 0.189980\n",
      "Cost after iteration 1228: 0.189785\n",
      "Cost after iteration 1230: 0.189591\n",
      "Cost after iteration 1232: 0.189397\n",
      "Cost after iteration 1234: 0.189204\n",
      "Cost after iteration 1236: 0.189011\n",
      "Cost after iteration 1238: 0.188818\n",
      "Cost after iteration 1240: 0.188626\n",
      "Cost after iteration 1242: 0.188434\n",
      "Cost after iteration 1244: 0.188243\n",
      "Cost after iteration 1246: 0.188052\n",
      "Cost after iteration 1248: 0.187861\n",
      "Cost after iteration 1250: 0.187671\n",
      "Cost after iteration 1252: 0.187481\n",
      "Cost after iteration 1254: 0.187291\n",
      "Cost after iteration 1256: 0.187102\n",
      "Cost after iteration 1258: 0.186913\n",
      "Cost after iteration 1260: 0.186725\n",
      "Cost after iteration 1262: 0.186537\n",
      "Cost after iteration 1264: 0.186349\n",
      "Cost after iteration 1266: 0.186162\n",
      "Cost after iteration 1268: 0.185975\n",
      "Cost after iteration 1270: 0.185789\n",
      "Cost after iteration 1272: 0.185602\n",
      "Cost after iteration 1274: 0.185417\n",
      "Cost after iteration 1276: 0.185231\n",
      "Cost after iteration 1278: 0.185046\n",
      "Cost after iteration 1280: 0.184861\n",
      "Cost after iteration 1282: 0.184677\n",
      "Cost after iteration 1284: 0.184493\n",
      "Cost after iteration 1286: 0.184309\n",
      "Cost after iteration 1288: 0.184126\n",
      "Cost after iteration 1290: 0.183943\n",
      "Cost after iteration 1292: 0.183760\n",
      "Cost after iteration 1294: 0.183578\n",
      "Cost after iteration 1296: 0.183396\n",
      "Cost after iteration 1298: 0.183215\n",
      "Cost after iteration 1300: 0.183033\n",
      "Cost after iteration 1302: 0.182852\n",
      "Cost after iteration 1304: 0.182672\n",
      "Cost after iteration 1306: 0.182492\n",
      "Cost after iteration 1308: 0.182312\n",
      "Cost after iteration 1310: 0.182133\n",
      "Cost after iteration 1312: 0.181953\n",
      "Cost after iteration 1314: 0.181775\n",
      "Cost after iteration 1316: 0.181596\n",
      "Cost after iteration 1318: 0.181418\n",
      "Cost after iteration 1320: 0.181240\n",
      "Cost after iteration 1322: 0.181063\n",
      "Cost after iteration 1324: 0.180886\n",
      "Cost after iteration 1326: 0.180709\n",
      "Cost after iteration 1328: 0.180533\n",
      "Cost after iteration 1330: 0.180357\n",
      "Cost after iteration 1332: 0.180181\n",
      "Cost after iteration 1334: 0.180006\n",
      "Cost after iteration 1336: 0.179831\n",
      "Cost after iteration 1338: 0.179656\n",
      "Cost after iteration 1340: 0.179481\n",
      "Cost after iteration 1342: 0.179307\n",
      "Cost after iteration 1344: 0.179134\n",
      "Cost after iteration 1346: 0.178960\n",
      "Cost after iteration 1348: 0.178787\n",
      "Cost after iteration 1350: 0.178614\n",
      "Cost after iteration 1352: 0.178442\n",
      "Cost after iteration 1354: 0.178270\n",
      "Cost after iteration 1356: 0.178098\n",
      "Cost after iteration 1358: 0.177927\n",
      "Cost after iteration 1360: 0.177755\n",
      "Cost after iteration 1362: 0.177585\n",
      "Cost after iteration 1364: 0.177414\n",
      "Cost after iteration 1366: 0.177244\n",
      "Cost after iteration 1368: 0.177074\n",
      "Cost after iteration 1370: 0.176905\n",
      "Cost after iteration 1372: 0.176735\n",
      "Cost after iteration 1374: 0.176566\n",
      "Cost after iteration 1376: 0.176398\n",
      "Cost after iteration 1378: 0.176229\n",
      "Cost after iteration 1380: 0.176061\n",
      "Cost after iteration 1382: 0.175894\n",
      "Cost after iteration 1384: 0.175726\n",
      "Cost after iteration 1386: 0.175559\n",
      "Cost after iteration 1388: 0.175393\n",
      "Cost after iteration 1390: 0.175226\n",
      "Cost after iteration 1392: 0.175060\n",
      "Cost after iteration 1394: 0.174894\n",
      "Cost after iteration 1396: 0.174729\n",
      "Cost after iteration 1398: 0.174564\n",
      "Cost after iteration 1400: 0.174399\n",
      "Cost after iteration 1402: 0.174234\n",
      "Cost after iteration 1404: 0.174070\n",
      "Cost after iteration 1406: 0.173906\n",
      "Cost after iteration 1408: 0.173742\n",
      "Cost after iteration 1410: 0.173579\n",
      "Cost after iteration 1412: 0.173415\n",
      "Cost after iteration 1414: 0.173253\n",
      "Cost after iteration 1416: 0.173090\n",
      "Cost after iteration 1418: 0.172928\n",
      "Cost after iteration 1420: 0.172766\n",
      "Cost after iteration 1422: 0.172604\n",
      "Cost after iteration 1424: 0.172443\n",
      "Cost after iteration 1426: 0.172282\n",
      "Cost after iteration 1428: 0.172121\n",
      "Cost after iteration 1430: 0.171961\n",
      "Cost after iteration 1432: 0.171801\n",
      "Cost after iteration 1434: 0.171641\n",
      "Cost after iteration 1436: 0.171481\n",
      "Cost after iteration 1438: 0.171322\n",
      "Cost after iteration 1440: 0.171163\n",
      "Cost after iteration 1442: 0.171004\n",
      "Cost after iteration 1444: 0.170845\n",
      "Cost after iteration 1446: 0.170687\n",
      "Cost after iteration 1448: 0.170529\n",
      "Cost after iteration 1450: 0.170372\n",
      "Cost after iteration 1452: 0.170214\n",
      "Cost after iteration 1454: 0.170057\n",
      "Cost after iteration 1456: 0.169901\n",
      "Cost after iteration 1458: 0.169744\n",
      "Cost after iteration 1460: 0.169588\n",
      "Cost after iteration 1462: 0.169432\n",
      "Cost after iteration 1464: 0.169276\n",
      "Cost after iteration 1466: 0.169121\n",
      "Cost after iteration 1468: 0.168966\n",
      "Cost after iteration 1470: 0.168811\n",
      "Cost after iteration 1472: 0.168657\n",
      "Cost after iteration 1474: 0.168502\n",
      "Cost after iteration 1476: 0.168348\n",
      "Cost after iteration 1478: 0.168195\n",
      "Cost after iteration 1480: 0.168041\n",
      "Cost after iteration 1482: 0.167888\n",
      "Cost after iteration 1484: 0.167735\n",
      "Cost after iteration 1486: 0.167582\n",
      "Cost after iteration 1488: 0.167430\n",
      "Cost after iteration 1490: 0.167278\n",
      "Cost after iteration 1492: 0.167126\n",
      "Cost after iteration 1494: 0.166975\n",
      "Cost after iteration 1496: 0.166823\n",
      "Cost after iteration 1498: 0.166672\n",
      "Cost after iteration 1500: 0.166521\n",
      "Cost after iteration 1502: 0.166371\n",
      "Cost after iteration 1504: 0.166221\n",
      "Cost after iteration 1506: 0.166071\n",
      "Cost after iteration 1508: 0.165921\n",
      "Cost after iteration 1510: 0.165771\n",
      "Cost after iteration 1512: 0.165622\n",
      "Cost after iteration 1514: 0.165473\n",
      "Cost after iteration 1516: 0.165325\n",
      "Cost after iteration 1518: 0.165176\n",
      "Cost after iteration 1520: 0.165028\n",
      "Cost after iteration 1522: 0.164880\n",
      "Cost after iteration 1524: 0.164732\n",
      "Cost after iteration 1526: 0.164585\n",
      "Cost after iteration 1528: 0.164438\n",
      "Cost after iteration 1530: 0.164291\n",
      "Cost after iteration 1532: 0.164144\n",
      "Cost after iteration 1534: 0.163998\n",
      "Cost after iteration 1536: 0.163852\n",
      "Cost after iteration 1538: 0.163706\n",
      "Cost after iteration 1540: 0.163560\n",
      "Cost after iteration 1542: 0.163415\n",
      "Cost after iteration 1544: 0.163270\n",
      "Cost after iteration 1546: 0.163125\n",
      "Cost after iteration 1548: 0.162980\n",
      "Cost after iteration 1550: 0.162836\n",
      "Cost after iteration 1552: 0.162692\n",
      "Cost after iteration 1554: 0.162548\n",
      "Cost after iteration 1556: 0.162404\n",
      "Cost after iteration 1558: 0.162261\n",
      "Cost after iteration 1560: 0.162118\n",
      "Cost after iteration 1562: 0.161975\n",
      "Cost after iteration 1564: 0.161832\n",
      "Cost after iteration 1566: 0.161689\n",
      "Cost after iteration 1568: 0.161547\n",
      "Cost after iteration 1570: 0.161405\n",
      "Cost after iteration 1572: 0.161264\n",
      "Cost after iteration 1574: 0.161122\n",
      "Cost after iteration 1576: 0.160981\n",
      "Cost after iteration 1578: 0.160840\n",
      "Cost after iteration 1580: 0.160699\n",
      "Cost after iteration 1582: 0.160559\n",
      "Cost after iteration 1584: 0.160418\n",
      "Cost after iteration 1586: 0.160278\n",
      "Cost after iteration 1588: 0.160138\n",
      "Cost after iteration 1590: 0.159999\n",
      "Cost after iteration 1592: 0.159860\n",
      "Cost after iteration 1594: 0.159720\n",
      "Cost after iteration 1596: 0.159582\n",
      "Cost after iteration 1598: 0.159443\n",
      "Cost after iteration 1600: 0.159305\n",
      "Cost after iteration 1602: 0.159166\n",
      "Cost after iteration 1604: 0.159028\n",
      "Cost after iteration 1606: 0.158891\n",
      "Cost after iteration 1608: 0.158753\n",
      "Cost after iteration 1610: 0.158616\n",
      "Cost after iteration 1612: 0.158479\n",
      "Cost after iteration 1614: 0.158342\n",
      "Cost after iteration 1616: 0.158206\n",
      "Cost after iteration 1618: 0.158069\n",
      "Cost after iteration 1620: 0.157933\n",
      "Cost after iteration 1622: 0.157797\n",
      "Cost after iteration 1624: 0.157661\n",
      "Cost after iteration 1626: 0.157526\n",
      "Cost after iteration 1628: 0.157391\n",
      "Cost after iteration 1630: 0.157256\n",
      "Cost after iteration 1632: 0.157121\n",
      "Cost after iteration 1634: 0.156986\n",
      "Cost after iteration 1636: 0.156852\n",
      "Cost after iteration 1638: 0.156718\n",
      "Cost after iteration 1640: 0.156584\n",
      "Cost after iteration 1642: 0.156450\n",
      "Cost after iteration 1644: 0.156317\n",
      "Cost after iteration 1646: 0.156184\n",
      "Cost after iteration 1648: 0.156051\n",
      "Cost after iteration 1650: 0.155918\n",
      "Cost after iteration 1652: 0.155785\n",
      "Cost after iteration 1654: 0.155653\n",
      "Cost after iteration 1656: 0.155521\n",
      "Cost after iteration 1658: 0.155389\n",
      "Cost after iteration 1660: 0.155257\n",
      "Cost after iteration 1662: 0.155126\n",
      "Cost after iteration 1664: 0.154994\n",
      "Cost after iteration 1666: 0.154863\n",
      "Cost after iteration 1668: 0.154733\n",
      "Cost after iteration 1670: 0.154602\n",
      "Cost after iteration 1672: 0.154471\n",
      "Cost after iteration 1674: 0.154341\n",
      "Cost after iteration 1676: 0.154211\n",
      "Cost after iteration 1678: 0.154081\n",
      "Cost after iteration 1680: 0.153952\n",
      "Cost after iteration 1682: 0.153822\n",
      "Cost after iteration 1684: 0.153693\n",
      "Cost after iteration 1686: 0.153564\n",
      "Cost after iteration 1688: 0.153436\n",
      "Cost after iteration 1690: 0.153307\n",
      "Cost after iteration 1692: 0.153179\n",
      "Cost after iteration 1694: 0.153050\n",
      "Cost after iteration 1696: 0.152923\n",
      "Cost after iteration 1698: 0.152795\n",
      "Cost after iteration 1700: 0.152667\n",
      "Cost after iteration 1702: 0.152540\n",
      "Cost after iteration 1704: 0.152413\n",
      "Cost after iteration 1706: 0.152286\n",
      "Cost after iteration 1708: 0.152159\n",
      "Cost after iteration 1710: 0.152033\n",
      "Cost after iteration 1712: 0.151906\n",
      "Cost after iteration 1714: 0.151780\n",
      "Cost after iteration 1716: 0.151654\n",
      "Cost after iteration 1718: 0.151529\n",
      "Cost after iteration 1720: 0.151403\n",
      "Cost after iteration 1722: 0.151278\n",
      "Cost after iteration 1724: 0.151153\n",
      "Cost after iteration 1726: 0.151028\n",
      "Cost after iteration 1728: 0.150903\n",
      "Cost after iteration 1730: 0.150779\n",
      "Cost after iteration 1732: 0.150655\n",
      "Cost after iteration 1734: 0.150530\n",
      "Cost after iteration 1736: 0.150406\n",
      "Cost after iteration 1738: 0.150283\n",
      "Cost after iteration 1740: 0.150159\n",
      "Cost after iteration 1742: 0.150036\n",
      "Cost after iteration 1744: 0.149913\n",
      "Cost after iteration 1746: 0.149790\n",
      "Cost after iteration 1748: 0.149667\n",
      "Cost after iteration 1750: 0.149545\n",
      "Cost after iteration 1752: 0.149422\n",
      "Cost after iteration 1754: 0.149300\n",
      "Cost after iteration 1756: 0.149178\n",
      "Cost after iteration 1758: 0.149056\n",
      "Cost after iteration 1760: 0.148935\n",
      "Cost after iteration 1762: 0.148813\n",
      "Cost after iteration 1764: 0.148692\n",
      "Cost after iteration 1766: 0.148571\n",
      "Cost after iteration 1768: 0.148450\n",
      "Cost after iteration 1770: 0.148330\n",
      "Cost after iteration 1772: 0.148209\n",
      "Cost after iteration 1774: 0.148089\n",
      "Cost after iteration 1776: 0.147969\n",
      "Cost after iteration 1778: 0.147849\n",
      "Cost after iteration 1780: 0.147729\n",
      "Cost after iteration 1782: 0.147610\n",
      "Cost after iteration 1784: 0.147490\n",
      "Cost after iteration 1786: 0.147371\n",
      "Cost after iteration 1788: 0.147252\n",
      "Cost after iteration 1790: 0.147133\n",
      "Cost after iteration 1792: 0.147015\n",
      "Cost after iteration 1794: 0.146896\n",
      "Cost after iteration 1796: 0.146778\n",
      "Cost after iteration 1798: 0.146660\n",
      "Cost after iteration 1800: 0.146542\n",
      "Cost after iteration 1802: 0.146425\n",
      "Cost after iteration 1804: 0.146307\n",
      "Cost after iteration 1806: 0.146190\n",
      "Cost after iteration 1808: 0.146073\n",
      "Cost after iteration 1810: 0.145956\n",
      "Cost after iteration 1812: 0.145839\n",
      "Cost after iteration 1814: 0.145722\n",
      "Cost after iteration 1816: 0.145606\n",
      "Cost after iteration 1818: 0.145490\n",
      "Cost after iteration 1820: 0.145373\n",
      "Cost after iteration 1822: 0.145258\n",
      "Cost after iteration 1824: 0.145142\n",
      "Cost after iteration 1826: 0.145026\n",
      "Cost after iteration 1828: 0.144911\n",
      "Cost after iteration 1830: 0.144796\n",
      "Cost after iteration 1832: 0.144681\n",
      "Cost after iteration 1834: 0.144566\n",
      "Cost after iteration 1836: 0.144451\n",
      "Cost after iteration 1838: 0.144337\n",
      "Cost after iteration 1840: 0.144222\n",
      "Cost after iteration 1842: 0.144108\n",
      "Cost after iteration 1844: 0.143994\n",
      "Cost after iteration 1846: 0.143881\n",
      "Cost after iteration 1848: 0.143767\n",
      "Cost after iteration 1850: 0.143654\n",
      "Cost after iteration 1852: 0.143540\n",
      "Cost after iteration 1854: 0.143427\n",
      "Cost after iteration 1856: 0.143314\n",
      "Cost after iteration 1858: 0.143201\n",
      "Cost after iteration 1860: 0.143089\n",
      "Cost after iteration 1862: 0.142976\n",
      "Cost after iteration 1864: 0.142864\n",
      "Cost after iteration 1866: 0.142752\n",
      "Cost after iteration 1868: 0.142640\n",
      "Cost after iteration 1870: 0.142528\n",
      "Cost after iteration 1872: 0.142417\n",
      "Cost after iteration 1874: 0.142305\n",
      "Cost after iteration 1876: 0.142194\n",
      "Cost after iteration 1878: 0.142083\n",
      "Cost after iteration 1880: 0.141972\n",
      "Cost after iteration 1882: 0.141861\n",
      "Cost after iteration 1884: 0.141751\n",
      "Cost after iteration 1886: 0.141640\n",
      "Cost after iteration 1888: 0.141530\n",
      "Cost after iteration 1890: 0.141420\n",
      "Cost after iteration 1892: 0.141310\n",
      "Cost after iteration 1894: 0.141200\n",
      "Cost after iteration 1896: 0.141091\n",
      "Cost after iteration 1898: 0.140981\n",
      "Cost after iteration 1900: 0.140872\n",
      "Cost after iteration 1902: 0.140763\n",
      "Cost after iteration 1904: 0.140654\n",
      "Cost after iteration 1906: 0.140545\n",
      "Cost after iteration 1908: 0.140437\n",
      "Cost after iteration 1910: 0.140328\n",
      "Cost after iteration 1912: 0.140220\n",
      "Cost after iteration 1914: 0.140112\n",
      "Cost after iteration 1916: 0.140004\n",
      "Cost after iteration 1918: 0.139896\n",
      "Cost after iteration 1920: 0.139788\n",
      "Cost after iteration 1922: 0.139681\n",
      "Cost after iteration 1924: 0.139573\n",
      "Cost after iteration 1926: 0.139466\n",
      "Cost after iteration 1928: 0.139359\n",
      "Cost after iteration 1930: 0.139252\n",
      "Cost after iteration 1932: 0.139146\n",
      "Cost after iteration 1934: 0.139039\n",
      "Cost after iteration 1936: 0.138933\n",
      "Cost after iteration 1938: 0.138826\n",
      "Cost after iteration 1940: 0.138720\n",
      "Cost after iteration 1942: 0.138614\n",
      "Cost after iteration 1944: 0.138508\n",
      "Cost after iteration 1946: 0.138403\n",
      "Cost after iteration 1948: 0.138297\n",
      "Cost after iteration 1950: 0.138192\n",
      "Cost after iteration 1952: 0.138087\n",
      "Cost after iteration 1954: 0.137982\n",
      "Cost after iteration 1956: 0.137877\n",
      "Cost after iteration 1958: 0.137772\n",
      "Cost after iteration 1960: 0.137668\n",
      "Cost after iteration 1962: 0.137563\n",
      "Cost after iteration 1964: 0.137459\n",
      "Cost after iteration 1966: 0.137355\n",
      "Cost after iteration 1968: 0.137251\n",
      "Cost after iteration 1970: 0.137147\n",
      "Cost after iteration 1972: 0.137044\n",
      "Cost after iteration 1974: 0.136940\n",
      "Cost after iteration 1976: 0.136837\n",
      "Cost after iteration 1978: 0.136734\n",
      "Cost after iteration 1980: 0.136630\n",
      "Cost after iteration 1982: 0.136528\n",
      "Cost after iteration 1984: 0.136425\n",
      "Cost after iteration 1986: 0.136322\n",
      "Cost after iteration 1988: 0.136220\n",
      "Cost after iteration 1990: 0.136117\n",
      "Cost after iteration 1992: 0.136015\n",
      "Cost after iteration 1994: 0.135913\n",
      "Cost after iteration 1996: 0.135811\n",
      "Cost after iteration 1998: 0.135710\n",
      "train accuracy: 99.04306220095694 %\n",
      "test accuracy: 70.0 %\n"
     ]
    }
   ],
   "source": [
    "logistic_regression_model = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations=2000, learning_rate=0.005, print_cost=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: Training accuracy is close to 100%. This is a good sanity check: your model is working and has high enough capacity to fit the training data. Test accuracy is 70%. It is actually not bad for this simple model, given the small dataset we used and that logistic regression is a linear classifier. But no worries, you'll build an even better classifier next week!\n",
    "\n",
    "Also, you see that the model is clearly overfitting the training data. Later in this specialization you will learn how to reduce overfitting, for example by using regularization. Using the code below (and changing the `index` variable) you can look at predictions on pictures of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 1, you predicted that it is a \"cat\" picture.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of a picture that was wrongly classified.\n",
    "index = 20\n",
    "plt.imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n",
    "print (\"y = \" + str(test_set_y[0,index]) + \", you predicted that it is a \\\"\" + classes[int(logistic_regression_model['Y_prediction_test'][0,index])].decode(\"utf-8\") +  \"\\\" picture.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also plot the cost function and the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.69314718 0.75315358 0.7685635  0.75561323 0.75148527 0.74494036\n",
      " 0.73953808 0.73411404 0.72898832 0.72403006 0.71924893 0.71461785\n",
      " 0.71012371 0.70575259 0.70149324 0.69733578 0.69327171 0.68929362\n",
      " 0.68539503 0.68157024 0.67781423 0.67412253 0.67049115 0.66691654\n",
      " 0.66339548 0.65992505 0.65650261 0.65312577 0.6497923  0.64650019\n",
      " 0.64324758 0.64003273 0.63685405 0.63371005 0.63059936 0.62752067\n",
      " 0.62447278 0.62145457 0.61846496 0.61550296 0.61256763 0.60965809\n",
      " 0.6067735  0.60391306 0.60107603 0.59826171 0.59546942 0.59269853\n",
      " 0.58994844 0.58721856 0.58450836 0.58181732 0.57914494 0.57649076\n",
      " 0.57385432 0.5712352  0.56863299 0.56604731 0.56347778 0.56092404\n",
      " 0.55838576 0.55586261 0.55335429 0.5508605  0.54838094 0.54591536\n",
      " 0.54346349 0.54102508 0.53859988 0.53618768 0.53378826 0.53140139\n",
      " 0.52902688 0.52666453 0.52431416 0.52197559 0.51964864 0.51733316\n",
      " 0.51502898 0.51273595 0.51045393 0.50818277 0.50592235 0.50367254\n",
      " 0.5014332  0.49920423 0.49698551 0.49477694 0.49257842 0.49038984\n",
      " 0.48821112 0.48604216 0.48388289 0.48173322 0.47959309 0.47746243\n",
      " 0.47534117 0.47322925 0.47112662 0.46903323 0.46694904 0.46487401\n",
      " 0.4628081  0.46075129 0.45870355 0.45666487 0.45463524 0.45261465\n",
      " 0.45060311 0.44860062 0.4466072  0.44462287 0.44264767 0.44068163\n",
      " 0.4387248  0.43677724 0.43483902 0.43291021 0.4309909  0.42908119\n",
      " 0.42718119 0.42529102 0.42341083 0.42154077 0.41968099 0.4178317\n",
      " 0.41599308 0.41416536 0.41234878 0.41054361 0.40875011 0.40696859\n",
      " 0.4051994  0.40344287 0.4016994  0.39996939 0.39825327 0.39655153\n",
      " 0.39486466 0.39319319 0.39153768 0.38989874 0.38827698 0.38667308\n",
      " 0.38508771 0.38352161 0.38197551 0.38045019 0.37894644 0.37746506\n",
      " 0.37600687 0.37457267 0.37316329 0.37177949 0.37042206 0.36909172\n",
      " 0.36778913 0.3665149  0.36526956 0.36405354 0.36286718 0.36171066\n",
      " 0.36058408 0.35948735 0.35842026 0.35738242 0.35637331 0.35539224\n",
      " 0.35443837 0.35351072 0.35260819 0.35172956 0.35087353 0.35003873\n",
      " 0.34922375 0.34842714 0.34764747 0.34688333 0.34613334 0.3453962\n",
      " 0.34467067 0.34395559 0.34324991 0.34255267 0.34186301 0.34118016\n",
      " 0.34050344 0.33983228 0.33916618 0.33850469 0.33784747 0.33719422\n",
      " 0.33654467 0.33589862 0.33525591 0.33461639 0.33397994 0.33334648\n",
      " 0.33271593 0.33208821 0.33146329 0.33084111 0.33022164 0.32960484\n",
      " 0.32899069 0.32837915 0.3277702  0.32716382 0.32656    0.3259587\n",
      " 0.3253599  0.3247636  0.32416977 0.32357838 0.32298944 0.3224029\n",
      " 0.32181877 0.32123702 0.32065763 0.32008059 0.31950588 0.31893348\n",
      " 0.31836339 0.31779557 0.31723003 0.31666673 0.31610567 0.31554683\n",
      " 0.31499019 0.31443574 0.31388347 0.31333336 0.31278539 0.31223955\n",
      " 0.31169583 0.31115422 0.31061469 0.31007723 0.30954184 0.3090085\n",
      " 0.30847719 0.30794789 0.30742061 0.30689532 0.30637201 0.30585067\n",
      " 0.30533129 0.30481385 0.30429834 0.30378475 0.30327307 0.30276328\n",
      " 0.30225537 0.30174933 0.30124515 0.30074282 0.30024233 0.29974365\n",
      " 0.29924679 0.29875174 0.29825847 0.29776698 0.29727726 0.2967893\n",
      " 0.29630308 0.2958186  0.29533585 0.29485481 0.29437548 0.29389784\n",
      " 0.29342189 0.29294761 0.29247499 0.29200403 0.29153472 0.29106703\n",
      " 0.29060098 0.29013654 0.28967371 0.28921247 0.28875282 0.28829475\n",
      " 0.28783825 0.28738331 0.28692992 0.28647807 0.28602776 0.28557897\n",
      " 0.2851317  0.28468593 0.28424166 0.28379888 0.28335759 0.28291777\n",
      " 0.28247941 0.28204251 0.28160706 0.28117305 0.28074047 0.28030932\n",
      " 0.27987959 0.27945126 0.27902434 0.2785988  0.27817466 0.27775189\n",
      " 0.2773305  0.27691047 0.27649179 0.27607446 0.27565848 0.27524383\n",
      " 0.2748305  0.2744185  0.27400781 0.27359842 0.27319034 0.27278354\n",
      " 0.27237804 0.27197381 0.27157085 0.27116916 0.27076872 0.27036954\n",
      " 0.2699716  0.26957491 0.26917944 0.2687852  0.26839218 0.26800038\n",
      " 0.26760978 0.26722038 0.26683218 0.26644516 0.26605933 0.26567468\n",
      " 0.2652912  0.26490888 0.26452772 0.26414771 0.26376886 0.26339114\n",
      " 0.26301456 0.26263911 0.26226478 0.26189158 0.26151948 0.2611485\n",
      " 0.26077862 0.26040983 0.26004214 0.25967553 0.25931    0.25894555\n",
      " 0.25858217 0.25821986 0.25785861 0.25749841 0.25713926 0.25678115\n",
      " 0.25642409 0.25606806 0.25571306 0.25535909 0.25500613 0.2546542\n",
      " 0.25430327 0.25395335 0.25360443 0.2532565  0.25290957 0.25256362\n",
      " 0.25221866 0.25187467 0.25153166 0.25118962 0.25084854 0.25050842\n",
      " 0.25016925 0.24983104 0.24949377 0.24915744 0.24882206 0.2484876\n",
      " 0.24815408 0.24782148 0.2474898  0.24715904 0.24682919 0.24650024\n",
      " 0.24617221 0.24584507 0.24551883 0.24519348 0.24486902 0.24454544\n",
      " 0.24422275 0.24390093 0.24357998 0.2432599  0.24294068 0.24262233\n",
      " 0.24230483 0.24198819 0.2416724  0.24135745 0.24104335 0.24073008\n",
      " 0.24041765 0.24010605 0.23979527 0.23948532 0.23917619 0.23886788\n",
      " 0.23856038 0.2382537  0.23794781 0.23764274 0.23733846 0.23703497\n",
      " 0.23673228 0.23643038 0.23612927 0.23582893 0.23552938 0.23523061\n",
      " 0.2349326  0.23463537 0.2343389  0.2340432  0.23374825 0.23345406\n",
      " 0.23316063 0.23286795 0.23257601 0.23228482 0.23199437 0.23170466\n",
      " 0.23141568 0.23112744 0.23083992 0.23055313 0.23026706 0.22998172\n",
      " 0.22969709 0.22941318 0.22912997 0.22884748 0.22856569 0.22828461\n",
      " 0.22800422 0.22772453 0.22744554 0.22716724 0.22688963 0.2266127\n",
      " 0.22633646 0.2260609  0.22578601 0.2255118  0.22523827 0.2249654\n",
      " 0.2246932  0.22442167 0.2241508  0.22388059 0.22361103 0.22334213\n",
      " 0.22307388 0.22280628 0.22253933 0.22227302 0.22200735 0.22174232\n",
      " 0.22147793 0.22121418 0.22095105 0.22068856 0.22042669 0.22016545\n",
      " 0.21990482 0.21964482 0.21938544 0.21912667 0.21886852 0.21861097\n",
      " 0.21835403 0.2180977  0.21784198 0.21758685 0.21733232 0.21707839\n",
      " 0.21682506 0.21657231 0.21632016 0.2160686  0.21581762 0.21556722\n",
      " 0.21531741 0.21506817 0.21481951 0.21457143 0.21432392 0.21407698\n",
      " 0.21383061 0.21358481 0.21333957 0.21309489 0.21285077 0.21260722\n",
      " 0.21236421 0.21212177 0.21187987 0.21163853 0.21139773 0.21115748\n",
      " 0.21091777 0.21067861 0.21043998 0.2102019  0.20996435 0.20972734\n",
      " 0.20949085 0.2092549  0.20901948 0.20878459 0.20855022 0.20831637\n",
      " 0.20808305 0.20785024 0.20761795 0.20738618 0.20715493 0.20692418\n",
      " 0.20669395 0.20646422 0.206235   0.20600629 0.20577808 0.20555037\n",
      " 0.20532316 0.20509645 0.20487024 0.20464452 0.20441929 0.20419456\n",
      " 0.20397031 0.20374655 0.20352328 0.20330049 0.20307819 0.20285637\n",
      " 0.20263503 0.20241416 0.20219377 0.20197386 0.20175442 0.20153545\n",
      " 0.20131695 0.20109892 0.20088136 0.20066426 0.20044762 0.20023145\n",
      " 0.20001574 0.19980048 0.19958569 0.19937135 0.19915746 0.19894403\n",
      " 0.19873104 0.19851851 0.19830643 0.19809479 0.1978836  0.19767285\n",
      " 0.19746255 0.19725269 0.19704326 0.19683427 0.19662573 0.19641761\n",
      " 0.19620993 0.19600268 0.19579587 0.19558948 0.19538352 0.19517799\n",
      " 0.19497288 0.1947682  0.19456393 0.1943601  0.19415668 0.19395368\n",
      " 0.19375109 0.19354892 0.19334717 0.19314583 0.1929449  0.19274439\n",
      " 0.19254428 0.19234458 0.19214528 0.1919464  0.19174791 0.19154983\n",
      " 0.19135215 0.19115487 0.19095799 0.19076151 0.19056542 0.19036973\n",
      " 0.19017443 0.18997953 0.18978501 0.18959089 0.18939716 0.18920381\n",
      " 0.18901085 0.18881828 0.18862609 0.18843428 0.18824286 0.18805181\n",
      " 0.18786115 0.18767086 0.18748095 0.18729142 0.18710226 0.18691348\n",
      " 0.18672507 0.18653703 0.18634936 0.18616206 0.18597512 0.18578856\n",
      " 0.18560236 0.18541652 0.18523105 0.18504594 0.18486119 0.1846768\n",
      " 0.18449277 0.1843091  0.18412579 0.18394283 0.18376022 0.18357797\n",
      " 0.18339608 0.18321453 0.18303334 0.18285249 0.182672   0.18249185\n",
      " 0.18231204 0.18213259 0.18195347 0.1817747  0.18159628 0.18141819\n",
      " 0.18124045 0.18106304 0.18088597 0.18070924 0.18053285 0.18035679\n",
      " 0.18018106 0.18000567 0.17983061 0.17965589 0.17948149 0.17930742\n",
      " 0.17913368 0.17896027 0.17878719 0.17861443 0.17844199 0.17826988\n",
      " 0.17809809 0.17792663 0.17775548 0.17758466 0.17741415 0.17724397\n",
      " 0.1770741  0.17690454 0.1767353  0.17656638 0.17639777 0.17622947\n",
      " 0.17606149 0.17589381 0.17572645 0.17555939 0.17539265 0.17522621\n",
      " 0.17506008 0.17489425 0.17472873 0.17456351 0.17439859 0.17423398\n",
      " 0.17406967 0.17390566 0.17374195 0.17357854 0.17341543 0.17325261\n",
      " 0.17309009 0.17292787 0.17276594 0.1726043  0.17244296 0.17228191\n",
      " 0.17212115 0.17196068 0.1718005  0.17164062 0.17148101 0.1713217\n",
      " 0.17116268 0.17100394 0.17084548 0.17068731 0.17052942 0.17037182\n",
      " 0.17021449 0.17005745 0.16990069 0.16974421 0.16958801 0.16943209\n",
      " 0.16927644 0.16912107 0.16896598 0.16881116 0.16865661 0.16850234\n",
      " 0.16834834 0.16819462 0.16804116 0.16788798 0.16773507 0.16758242\n",
      " 0.16743005 0.16727794 0.1671261  0.16697453 0.16682322 0.16667218\n",
      " 0.1665214  0.16637088 0.16622063 0.16607064 0.16592091 0.16577144\n",
      " 0.16562223 0.16547329 0.1653246  0.16517616 0.16502799 0.16488007\n",
      " 0.16473241 0.164585   0.16443785 0.16429095 0.16414431 0.16399792\n",
      " 0.16385178 0.16370589 0.16356025 0.16341486 0.16326973 0.16312484\n",
      " 0.16298019 0.1628358  0.16269165 0.16254775 0.1624041  0.16226068\n",
      " 0.16211752 0.16197459 0.16183191 0.16168948 0.16154728 0.16140533\n",
      " 0.16126361 0.16112214 0.1609809  0.1608399  0.16069915 0.16055863\n",
      " 0.16041834 0.16027829 0.16013848 0.1599989  0.15985956 0.15972045\n",
      " 0.15958157 0.15944293 0.15930452 0.15916634 0.15902839 0.15889067\n",
      " 0.15875318 0.15861592 0.15847889 0.15834209 0.15820551 0.15806916\n",
      " 0.15793304 0.15779714 0.15766147 0.15752602 0.15739079 0.15725579\n",
      " 0.15712102 0.15698646 0.15685213 0.15671802 0.15658412 0.15645045\n",
      " 0.156317   0.15618377 0.15605075 0.15591796 0.15578538 0.15565301\n",
      " 0.15552087 0.15538894 0.15525722 0.15512572 0.15499444 0.15486336\n",
      " 0.15473251 0.15460186 0.15447142 0.1543412  0.15421119 0.15408139\n",
      " 0.1539518  0.15382242 0.15369324 0.15356428 0.15343552 0.15330697\n",
      " 0.15317863 0.1530505  0.15292257 0.15279484 0.15266732 0.15254001\n",
      " 0.1524129  0.15228599 0.15215929 0.15203279 0.15190649 0.15178039\n",
      " 0.1516545  0.1515288  0.1514033  0.15127801 0.15115291 0.15102801\n",
      " 0.15090331 0.15077881 0.1506545  0.1505304  0.15040648 0.15028277\n",
      " 0.15015925 0.15003592 0.14991279 0.14978985 0.14966711 0.14954456\n",
      " 0.1494222  0.14930003 0.14917806 0.14905628 0.14893468 0.14881328\n",
      " 0.14869207 0.14857105 0.14845022 0.14832957 0.14820911 0.14808885\n",
      " 0.14796876 0.14784887 0.14772916 0.14760964 0.1474903  0.14737115\n",
      " 0.14725218 0.1471334  0.1470148  0.14689639 0.14677815 0.1466601\n",
      " 0.14654224 0.14642455 0.14630704 0.14618972 0.14607258 0.14595561\n",
      " 0.14583883 0.14572222 0.1456058  0.14548955 0.14537348 0.14525759\n",
      " 0.14514187 0.14502633 0.14491097 0.14479579 0.14468078 0.14456594\n",
      " 0.14445128 0.14433679 0.14422248 0.14410834 0.14399438 0.14388058\n",
      " 0.14376696 0.14365351 0.14354024 0.14342713 0.1433142  0.14320143\n",
      " 0.14308884 0.14297641 0.14286416 0.14275207 0.14264015 0.1425284\n",
      " 0.14241682 0.14230541 0.14219416 0.14208308 0.14197216 0.14186141\n",
      " 0.14175083 0.14164041 0.14153016 0.14142007 0.14131014 0.14120038\n",
      " 0.14109078 0.14098135 0.14087208 0.14076297 0.14065402 0.14054523\n",
      " 0.1404366  0.14032814 0.14021983 0.14011169 0.14000371 0.13989588\n",
      " 0.13978821 0.13968071 0.13957336 0.13946617 0.13935913 0.13925226\n",
      " 0.13914554 0.13903897 0.13893257 0.13882632 0.13872022 0.13861428\n",
      " 0.1385085  0.13840287 0.13829739 0.13819207 0.1380869  0.13798189\n",
      " 0.13787702 0.13777231 0.13766776 0.13756335 0.1374591  0.13735499\n",
      " 0.13725104 0.13714724 0.13704359 0.13694009 0.13683674 0.13673354\n",
      " 0.13663048 0.13652758 0.13642482 0.13632221 0.13621975 0.13611744\n",
      " 0.13601527 0.13591326 0.13581138 0.13570966]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot learning curve (with costs)\n",
    "costs = np.squeeze(logistic_regression_model['costs'])\n",
    "print(costs)\n",
    "plt.plot(costs)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per hundreds)')\n",
    "plt.title(\"Learning rate =\" + str(logistic_regression_model[\"learning_rate\"]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**:\n",
    "You can see the cost decreasing. It shows that the parameters are being learned. However, you see that you could train the model even more on the training set. Try to increase the number of iterations in the cell above and rerun the cells. You might see that the training set accuracy goes up, but the test set accuracy goes down. This is called overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6'></a>\n",
    "## 6 - Further analysis (optional/ungraded exercise) ##\n",
    "\n",
    "Congratulations on building your first image classification model. Let's analyze it further, and examine possible choices for the learning rate $\\alpha$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choice of learning rate ####\n",
    "\n",
    "**Reminder**:\n",
    "In order for Gradient Descent to work you must choose the learning rate wisely. The learning rate $\\alpha$  determines how rapidly we update the parameters. If the learning rate is too large we may \"overshoot\" the optimal value. Similarly, if it is too small we will need too many iterations to converge to the best values. That's why it is crucial to use a well-tuned learning rate.\n",
    "\n",
    "Let's compare the learning curve of our model with several choices of learning rates. Run the cell below. This should take about 1 minute. Feel free also to try different values than the three we have initialized the `learning_rates` variable to contain, and see what happens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "models = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print (\"Training a model with learning rate: \" + str(lr))\n",
    "    models[str(lr)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations=1500, learning_rate=lr, print_cost=False)\n",
    "    print ('\\n' + \"-------------------------------------------------------\" + '\\n')\n",
    "\n",
    "for lr in learning_rates:\n",
    "    plt.plot(np.squeeze(models[str(lr)][\"costs\"]), label=str(models[str(lr)][\"learning_rate\"]))\n",
    "\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (hundreds)')\n",
    "\n",
    "legend = plt.legend(loc='upper center', shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**: \n",
    "- Different learning rates give different costs and thus different predictions results.\n",
    "- If the learning rate is too large (0.01), the cost may oscillate up and down. It may even diverge (though in this example, using 0.01 still eventually ends up at a good value for the cost). \n",
    "- A lower cost doesn't mean a better model. You have to check if there is possibly overfitting. It happens when the training accuracy is a lot higher than the test accuracy.\n",
    "- In deep learning, we usually recommend that you: \n",
    "    - Choose the learning rate that better minimizes the cost function.\n",
    "    - If your model overfits, use other techniques to reduce overfitting. (We'll talk about this in later videos.) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='7'></a>\n",
    "## 7 - Test with your own image (optional/ungraded exercise) ##\n",
    "\n",
    "Congratulations on finishing this assignment. You can use your own image and see the output of your model. To do that:\n",
    "    1. Click on \"File\" in the upper bar of this notebook, then click \"Open\" to go on your Coursera Hub.\n",
    "    2. Add your image to this Jupyter Notebook's directory, in the \"images\" folder\n",
    "    3. Change your image's name in the following code\n",
    "    4. Run the code and check if the algorithm is right (1 = cat, 0 = non-cat)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this to the name of your image file\n",
    "my_image = \"my_image.jpg\"   \n",
    "\n",
    "# We preprocess the image to fit your algorithm.\n",
    "fname = \"images/\" + my_image\n",
    "image = np.array(Image.open(fname).resize((num_px, num_px)))\n",
    "plt.imshow(image)\n",
    "image = image / 255.\n",
    "image = image.reshape((1, num_px * num_px * 3)).T\n",
    "my_predicted_image = predict(logistic_regression_model[\"w\"], logistic_regression_model[\"b\"], image)\n",
    "\n",
    "print(\"y = \" + str(np.squeeze(my_predicted_image)) + \", your algorithm predicts a \\\"\" + classes[int(np.squeeze(my_predicted_image)),].decode(\"utf-8\") +  \"\\\" picture.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "**What to remember from this assignment:**\n",
    "1. Preprocessing the dataset is important.\n",
    "2. You implemented each function separately: initialize(), propagate(), optimize(). Then you built a model().\n",
    "3. Tuning the learning rate (which is an example of a \"hyperparameter\") can make a big difference to the algorithm. You will see more examples of this later in this course!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if you'd like, we invite you to try different things on this Notebook. Make sure you submit before trying anything. Once you submit, things you can play with include:\n",
    "    - Play with the learning rate and the number of iterations\n",
    "    - Try different initialization methods and compare the results\n",
    "    - Test other preprocessings (center the data, or divide each row by its standard deviation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bibliography:\n",
    "- http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/\n",
    "- https://stats.stackexchange.com/questions/211436/why-do-we-normalize-images-by-subtracting-the-datasets-image-mean-and-not-the-c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
